"""LangGraph ê¸°ë°˜ ì¥ë³´ê¸° AI ì—ì´ì „íŠ¸ (ReAct íŒ¨í„´)."""
from __future__ import annotations

import logging
import re
from typing import TypedDict, Annotated, Literal

from langchain_core.messages import SystemMessage, BaseMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

from src.application.prompts import render_prompt
from src.core.llm import ainvoke_json_with_model_fallback, is_openai_configured
from src.domain.models.basket import BasketItem, ItemMode
from src.api.v1.routers.basket import get_basket_store

logger = logging.getLogger(__name__)

ADD_KEYWORDS = ("ë‹´ì•„", "ì¶”ê°€", "ë„£ì–´", "ì‚¬ì¤˜", "ì£¼ë¬¸", "ì¥ë°”êµ¬ë‹ˆ")
REMOVE_KEYWORDS = ("ë¹¼ì¤˜", "ë¹¼", "ì‚­ì œ", "ì·¨ì†Œ", "ì§€ì›Œ", "ì œê±°")
RECOMMEND_KEYWORDS = ("ì¶”ì²œ", "ë­ ë¨¹", "ì–´ë–¤", "ë ˆì‹œí”¼")
ASK_KEYWORDS = ("ê·¸ê±°", "ì•„ê¹Œ", "ì–´ë–¤ ê±°")
QUANTITY_PATTERN = re.compile(r"(\d+)\s*(ê°œ|ë´‰|íŒ©|ì„¸íŠ¸|ë³‘|ìº”|í†µ|ì¤„|ë¬¶ìŒ)")
SIZE_PATTERN = re.compile(r"(\d+(?:\.\d+)?)\s*(kg|g|ml|l|êµ¬|íŒ|ëª¨|í¬ê¸°|ë‹¨)", re.IGNORECASE)
SEGMENT_SPLIT_PATTERN = re.compile(r"(?:,|/|\n| ê·¸ë¦¬ê³  |ê·¸ë¦¬ê³ | í•˜ê³  |í•˜ê³ | ë‘ |ë‘| ë° )")
KOREAN_QTY_WORDS = {
    "í•œ": 1,
    "í•˜ë‚˜": 1,
    "ë‘": 2,
    "ë‘˜": 2,
    "ì„¸": 3,
    "ì…‹": 3,
    "ë„¤": 4,
    "ë„·": 4,
}
KOREAN_QTY_PATTERN = re.compile(r"(í•œ|í•˜ë‚˜|ë‘|ë‘˜|ì„¸|ì…‹|ë„¤|ë„·)\s*(ê°œ|ë´‰|íŒ©|ì„¸íŠ¸|ë³‘|ìº”|í†µ|ì¤„|ë¬¶ìŒ|ëª¨)")
KNOWN_ITEM_ALIASES = {
    "ë‹¬ê±€": "ê³„ë€",
    "ê³„ë€": "ê³„ë€",
    "ìš°ìœ ": "ìš°ìœ ",
    "ë‘ë¶€": "ë‘ë¶€",
    "ì‚¼ê²¹ì‚´": "ì‚¼ê²¹ì‚´",
    "ì‚¼ê²¹": "ì‚¼ê²¹ì‚´",
    "ëª©ì‚´": "ë¼ì§€ê³ ê¸°",
    "ë¼ì§€ê³ ê¸°": "ë¼ì§€ê³ ê¸°",
    "ì†Œê³ ê¸°": "ì†Œê³ ê¸°",
    "ë‹­ê°€ìŠ´ì‚´": "ë‹­ê°€ìŠ´ì‚´",
    "ë¼ë©´": "ë¼ë©´",
    "ì‹ ë¼ë©´": "ì‹ ë¼ë©´",
    "ê¹€ì¹˜": "ê¹€ì¹˜",
    "ëŒ€íŒŒ": "ëŒ€íŒŒ",
    "íŒŒ": "íŒŒ",
    "ë‹¹ê·¼": "ë‹¹ê·¼",
    "ìƒì¶”": "ìƒì¶”",
    "ê¹»ì": "ê¹»ì",
    "ì¼€ì¼": "ì¼€ì¼",
    "ì˜¤ì´": "ì˜¤ì´",
    "ê³ ì¶”": "ê³ ì¶”",
    "ì–‘íŒŒ": "ì–‘íŒŒ",
    "ê°ì": "ê°ì",
    "ê³ êµ¬ë§ˆ": "ê³ êµ¬ë§ˆ",
    "ìƒìˆ˜": "ìƒìˆ˜",
    "ì½œë¼": "ì½œë¼",
    "ë§¥ì£¼": "ë§¥ì£¼",
    "ì†Œì£¼": "ì°¸ì´ìŠ¬",
    "ì°¸ì´ìŠ¬": "ì°¸ì´ìŠ¬",
    "ë¹„íƒ€500": "ë¹„íƒ€500",
    "ë¹„íƒ€ 500": "ë¹„íƒ€500",
}
KNOWN_BRANDS = (
    "ì„œìš¸ìš°ìœ ",
    "ë§¤ì¼",
    "ë‚¨ì–‘",
    "ë¹™ê·¸ë ˆ",
    "í’€ë¬´ì›",
    "cj",
    "cjì œì¼ì œë‹¹",
    "ì˜¤ëšœê¸°",
    "ë†ì‹¬",
    "ëª©ìš°ì´Œ",
    "í•œëˆ",
    "ë¡¯ë°",
    "ì§„ë¡œ",
)
STOPWORDS = {
    "ì¶”ê°€",
    "ì¶”ê°€í•´ì¤˜",
    "ë‹´ì•„",
    "ë‹´ì•„ì¤˜",
    "ë„£ì–´ì¤˜",
    "í•´ì¤˜",
    "ì£¼ì„¸ìš”",
    "ì¢€",
    "ì´ê±°",
    "ê·¸ê±°",
    "ì´ê²ƒ",
    "ì €ê±°",
    "ì›ì‚°ì§€",
    "ì›ì‚°ì§€ëŠ”",
    "ì–´ë””ë“ ",
    "ë”±íˆ",
    "ìƒê´€ì—†ê³ ",
    "ëƒ‰ë™ìœ¼ë¡œ",
    "ì‹¼ê±°",
    "ì •í™•íˆ",
    "ë¸Œëœë“œ",
    "ìš©ëŸ‰",
    "ê·œê²©",
    "ë‹´ê¸°",
    "ì¥ë°”êµ¬ë‹ˆì—",
    "ë„£ê¸°",
}
ITEM_LIQUID_DEFAULT_SIZE = {
    "ì°¸ì´ìŠ¬": "360ml",
    "ë¹„íƒ€500": "100ml",
    "ìš°ìœ ": "1l",
    "ì½œë¼": "1.5l",
    "ìƒìˆ˜": "2l",
}
RECIPE_BUNDLES: dict[str, list[tuple[str, int, str | None, str | None]]] = {
    "ê¹€ì¹˜ì°Œê°œ": [
        ("ê¹€ì¹˜", 1, None, "1/4í¬ê¸°"),
        ("ë‘ë¶€", 1, None, "1ëª¨"),
        ("ë¼ì§€ê³ ê¸°", 1, None, "200g"),
    ],
}


# â”€â”€ 1. State ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ChatState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    user_preferences: str
    matcher_entities: list[dict] | None
    intent: str | None
    next_step: str | None
    final_response: str | None
    user_id: str


# â”€â”€ 2. LLM ì„¤ì • (ëª¨ë¸ fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# â”€â”€ 3. Node êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def analyzer_node(state: ChatState) -> dict:
    """ì‚¬ìš©ì ì˜ë„ ë¶„ì„ ë…¸ë“œ."""
    basket = get_basket_store(state.get("user_id", "unknown_user"))

    last_msg = state["messages"][-1].content if state["messages"] else ""

    if not is_openai_configured():
        # API í‚¤ ì—†ìŒ â†’ í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ë¶„ë¥˜
        intent = _keyword_classify(last_msg)
        return {"intent": intent}

    basket_desc = ", ".join(
        f"{i.item_name}({i.brand or 'ì¶”ì²œ'})" for i in basket.items
    ) or "ë¹„ì–´ ìˆìŒ"

    prompt = render_prompt(
        "analyzer.system.txt",
        preferences=state["user_preferences"],
        basket_status=basket_desc,
    )

    try:
        messages = [SystemMessage(content=prompt)] + list(state["messages"])
        result = await ainvoke_json_with_model_fallback(messages, temperature=0.1)
        intent = _override_intent_with_item_heuristic(last_msg, result.get("intent", "general"))
        llm_entities = _normalize_llm_entities(result.get("entities"))
        merged_entities = _merge_matcher_entities(
            state.get("matcher_entities"),
            llm_entities,
        )
        return {
            "intent": intent,
            "matcher_entities": merged_entities,
        }
    except Exception as exc:
        logger.warning("LLM analyzer failed, fallback classifier used: %s", exc)
        return {"intent": _keyword_classify(last_msg)}


def _normalize_llm_entities(raw_entities: object) -> list[dict]:
    if not isinstance(raw_entities, list):
        return []

    normalized: list[dict] = []
    for raw in raw_entities:
        if not isinstance(raw, dict):
            continue

        item_name = str(raw.get("item_name") or "").strip()
        if not item_name:
            continue

        action = str(raw.get("action") or "add").strip().lower()
        if action not in {"add", "remove"}:
            action = "add"

        try:
            quantity = max(1, int(raw.get("quantity") or 1))
        except Exception:
            quantity = 1

        brand = str(raw.get("brand") or "").strip() or None
        size = str(raw.get("size") or "").strip() or None
        normalized.append(
            {
                "item_name": item_name,
                "quantity": quantity,
                "brand": brand,
                "size": size,
                "action": action,
            }
        )
    return normalized


def _merge_matcher_entities(
    base_entities: list[dict] | None,
    llm_entities: list[dict] | None,
) -> list[dict]:
    merged: dict[str, dict] = {}

    def _upsert(entity: dict) -> None:
        item_name = str(entity.get("item_name") or "").strip()
        if not item_name:
            return
        action = str(entity.get("action") or "add").strip().lower()
        if action not in {"add", "remove"}:
            action = "add"
        brand = str(entity.get("brand") or "").strip() or None
        size = str(entity.get("size") or "").strip() or None
        quantity = entity.get("quantity") or 1
        try:
            quantity_int = max(1, int(quantity))
        except Exception:
            quantity_int = 1

        key = f"{action}|{item_name.lower()}|{(brand or '').lower()}|{(size or '').lower()}"
        if key in merged:
            merged[key]["quantity"] = int(merged[key]["quantity"]) + quantity_int
            return
        merged[key] = {
            "item_name": item_name,
            "quantity": quantity_int,
            "brand": brand,
            "size": size,
            "action": action,
        }

    for entity in base_entities or []:
        if isinstance(entity, dict):
            _upsert(entity)
    for entity in llm_entities or []:
        if isinstance(entity, dict):
            _upsert(entity)

    return list(merged.values())


def _keyword_classify(text: str) -> str:
    """API í‚¤ ì—†ì„ ë•Œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ë¶„ë¥˜."""
    normalized = text.strip().lower()
    for kw in ADD_KEYWORDS + REMOVE_KEYWORDS:
        if kw in normalized:
            return "modify"
    if _extract_item_name(normalized):
        return "modify"
    for kw in RECOMMEND_KEYWORDS:
        if kw in normalized:
            return "recommend"
    for kw in ASK_KEYWORDS:
        if kw in normalized:
            return "clarify"
    return "general"


def _override_intent_with_item_heuristic(text: str, llm_intent: str) -> str:
    """LLMì´ general/clarifyë¡œ ë¶„ë¥˜í•´ë„ í’ˆëª© ë‹¨ë… ë°œí™”ë©´ modifyë¡œ ë³´ì •."""
    normalized = text.strip().lower()
    has_modify_signal = _extract_item_name(text) or any(
        keyword in normalized for keyword in (*ADD_KEYWORDS, *REMOVE_KEYWORDS)
    )
    if llm_intent in {"general", "clarify"} and has_modify_signal:
        return "modify"
    return llm_intent


async def modifier_node(state: ChatState) -> dict:
    """ì¥ë°”êµ¬ë‹ˆ ë³€ê²½ ì‹¤í–‰ ë…¸ë“œ â€” _basket_store ì§ì ‘ ì¡°ì‘."""
    last_msg = state["messages"][-1].content if state["messages"] else ""
    basket = get_basket_store(state.get("user_id", "unknown_user"))

    # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ íŒŒì‹± (MVP)
    action, item_name, quantity, brand, size, needs_followup, extra_items = _parse_modify_intent(
        last_msg,
        state.get("messages", []),
        state.get("matcher_entities"),
    )

    if action == "add":
        if needs_followup:
            return {"final_response": _build_item_followup(item_name)}

        items_to_apply = [(item_name, quantity, brand, size), *extra_items]
        added_labels: list[str] = []

        for cur_item_name, cur_quantity, cur_brand, cur_size in items_to_apply:
            existing_item = next(
                (
                    existing
                    for existing in basket.items
                    if existing.item_name == cur_item_name
                    and (existing.brand or "") == (cur_brand or "")
                    and (existing.size or "") == (cur_size or "")
                ),
                None,
            )

            if existing_item:
                existing_item.quantity += cur_quantity
            else:
                basket.items.append(
                    BasketItem(
                        item_name=cur_item_name,
                        brand=cur_brand,
                        size=cur_size,
                        quantity=cur_quantity,
                        mode=ItemMode.FIXED if cur_brand else ItemMode.RECOMMEND,
                    )
                )
            added_labels.append(_format_item_label(cur_item_name, cur_brand, cur_size))

        if len(added_labels) > 1:
            labels = ", ".join(added_labels)
            return {"final_response": f"{labels}ë¥¼ ì¥ë°”êµ¬ë‹ˆì— ë‹´ì•˜ì–´ìš”! ğŸ›’"}

        return {
            "final_response": f"{_format_item_label(item_name, brand, size)} {quantity}ê°œë¥¼ ì¥ë°”êµ¬ë‹ˆì— ë‹´ì•˜ì–´ìš”! ğŸ›’"
        }

    elif action == "remove":
        if not item_name:
            return {"final_response": "ì–´ë–¤ í’ˆëª©ì„ ëº„ì§€ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: 'ìš°ìœ  ë¹¼ì¤˜'"}
        before = len(basket.items)
        target = item_name.strip()
        basket.items = [i for i in basket.items if target not in i.item_name]
        if len(basket.items) < before:
            return {"final_response": f"'{item_name}'ì„(ë¥¼) ì¥ë°”êµ¬ë‹ˆì—ì„œ ëºì–´ìš”."}
        return {"final_response": f"'{item_name}'ì´(ê°€) ì¥ë°”êµ¬ë‹ˆì— ì—†ì–´ìš”."}

    return {"final_response": "ì¥ë°”êµ¬ë‹ˆë¥¼ ì–´ë–»ê²Œ ë³€ê²½í• ì§€ ì •í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: 'ê³„ë€ 30êµ¬ ë‹´ì•„ì¤˜'"}


def _parse_modify_intent(
    text: str,
    messages: list[BaseMessage],
    matcher_entities: list[dict] | None = None,
) -> tuple[str, str, int, str | None, str | None, bool, list[tuple[str, int, str | None, str | None]]]:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì¥ë°”êµ¬ë‹ˆ ë³€ê²½ íŒŒì‹±."""
    raw = text.strip()
    normalized = raw.lower()

    # ì‚­ì œ ê°ì§€
    for kw in REMOVE_KEYWORDS:
        if kw in normalized:
            prefix = raw.split(kw)[0].strip()
            item = _extract_item_name(prefix, allow_fallback=False) if prefix else None
            return "remove", item or "", 0, None, None, False, []

    parsed_items = _extract_recipe_bundle_items(raw)
    if not parsed_items:
        parsed_items = []
        for segment in _split_item_segments(raw):
            parsed = _parse_add_segment(segment)
            if parsed:
                parsed_items.append(parsed)

    # í•œ ë¬¸ì¥ ë‚´ ë‹¤ì¤‘ í’ˆëª©(ì˜ˆ: "ìš°ìœ  ê³„ë€ ë‘ë¶€ ë‹´ì•„ì¤˜") ì²˜ë¦¬
    mentioned_items = _extract_all_item_names(raw)
    if len(mentioned_items) > 1 and len(parsed_items) <= 1:
        parsed_items = []
        for mentioned in mentioned_items:
            parsed_items.append((mentioned, 1, _extract_brand(raw), _extract_size(raw, mentioned, 1)))

    # ì„¸ê·¸ë¨¼íŠ¸ íŒŒì‹±ì´ ì‹¤íŒ¨í–ˆì§€ë§Œ í’ˆëª©ì´ ëª…ì‹œë˜ì–´ ìˆìœ¼ë©´ ì „ì²´ ë¬¸ì¥ì—ì„œ ì¬ì‹œë„
    if not parsed_items:
        whole_item = _extract_item_name(raw, allow_fallback=True)
        if whole_item:
            qty = _extract_quantity(raw)
            brand = _extract_brand(raw)
            size = _extract_size(raw, whole_item, qty)
            parsed_items.append((whole_item, qty, brand, size))

    # ë¬¸ë§¥ ê¸°ë°˜ í›„ì† ë°œí™” ì²˜ë¦¬ (ì˜ˆ: "ëƒ‰ë™ìœ¼ë¡œ ì‹¼ê±° 1kg ì¶”ê°€í•´ì¤˜")
    if not parsed_items:
        inferred = _infer_recent_item_from_messages(messages)
        if inferred and _contains_add_details(raw):
            qty = _extract_quantity(raw)
            brand = _extract_brand(raw)
            size = _extract_size(raw, inferred, qty)
            parsed_items.append((inferred, qty, brand, size))

    # DB matcher/LLM ì—”í„°í‹° ê¸°ë°˜ ë³´ê°• (chat router + analyzerì—ì„œ ì£¼ì…í•œ í›„ë³´ ì‚¬ìš©)
    if not parsed_items and matcher_entities:
        remove_entities = [
            entity
            for entity in matcher_entities
            if str(entity.get("action") or "").strip().lower() == "remove"
            and str(entity.get("item_name") or "").strip()
        ]
        if remove_entities:
            target = str(remove_entities[0].get("item_name")).strip()
            return "remove", target, 0, None, None, False, []

        for entity in matcher_entities:
            if str(entity.get("action") or "add").strip().lower() == "remove":
                continue
            try:
                entity_quantity = max(1, int(entity.get("quantity", 1)))
            except Exception:
                entity_quantity = 1
            parsed_items.append(
                (
                    str(entity.get("item_name", "")).strip(),
                    entity_quantity,
                    entity.get("brand"),
                    entity.get("size"),
                )
            )
        parsed_items = [item for item in parsed_items if item[0]]

    if not parsed_items:
        fallback_item = _extract_item_name(raw, allow_fallback=True)
        return "add", fallback_item or "ì•Œ ìˆ˜ ì—†ëŠ” í’ˆëª©", 1, None, None, True, []

    # ì¤‘ë³µ í’ˆëª©ì€ ìˆ˜ëŸ‰ í•©ì‚°
    merged: dict[tuple[str, str, str], tuple[str, int, str | None, str | None]] = {}
    for item_name, qty, brand, size in parsed_items:
        key = (item_name, brand or "", size or "")
        if key in merged:
            prev_item, prev_qty, prev_brand, prev_size = merged[key]
            merged[key] = (prev_item, prev_qty + qty, prev_brand, prev_size)
        else:
            merged[key] = (item_name, qty, brand, size)

    merged_items = list(merged.values())
    first_item_name, first_qty, first_brand, first_size = merged_items[0]
    extra_items = merged_items[1:]
    needs_followup = (
        len(merged_items) == 1
        and not any(kw in normalized for kw in ADD_KEYWORDS)
        and not _contains_add_details(raw)
    )
    return "add", first_item_name, first_qty, first_brand, first_size, needs_followup, extra_items


def _extract_recipe_bundle_items(text: str) -> list[tuple[str, int, str | None, str | None]]:
    normalized = text.strip().lower()
    if "ê¹€ì¹˜ì°Œê°œ" in normalized and "ì¬ë£Œ" in normalized and any(
        keyword in normalized for keyword in ADD_KEYWORDS
    ):
        return list(RECIPE_BUNDLES["ê¹€ì¹˜ì°Œê°œ"])
    return []


def _split_item_segments(text: str) -> list[str]:
    segments = [segment.strip() for segment in SEGMENT_SPLIT_PATTERN.split(text) if segment.strip()]
    return segments or [text.strip()]


def _extract_all_item_names(text: str) -> list[str]:
    normalized = text.lower()
    hits: list[tuple[int, str]] = []
    for alias, canonical in KNOWN_ITEM_ALIASES.items():
        index = normalized.find(alias.lower())
        if index >= 0:
            hits.append((index, canonical))
    if not hits:
        return []

    hits.sort(key=lambda item: item[0])
    unique: list[str] = []
    for _, canonical in hits:
        if canonical not in unique:
            unique.append(canonical)
    return unique


def _parse_add_segment(segment: str) -> tuple[str, int, str | None, str | None] | None:
    item_name = _extract_item_name(segment, allow_fallback=True)
    if not item_name:
        return None
    qty = _extract_quantity(segment)
    brand = _extract_brand(segment)
    size = _extract_size(segment, item_name, qty)
    return item_name, qty, brand, size


def _extract_quantity(text: str) -> int:
    qty_match = QUANTITY_PATTERN.search(text)
    if qty_match:
        return int(qty_match.group(1))

    kor_match = KOREAN_QTY_PATTERN.search(text)
    if kor_match:
        return KOREAN_QTY_WORDS.get(kor_match.group(1), 1)
    return 1


def _extract_item_name(text: str, allow_fallback: bool = False) -> str | None:
    normalized = text.strip().lower()
    for alias, canonical in sorted(KNOWN_ITEM_ALIASES.items(), key=lambda x: len(x[0]), reverse=True):
        if alias in normalized:
            return canonical

    if allow_fallback:
        tokens = re.findall(r"[ê°€-í£a-zA-Z0-9]+", text)
        for token in tokens:
            lowered = token.lower()
            if lowered in STOPWORDS:
                continue
            if re.fullmatch(r"\d+(?:\.\d+)?", lowered):
                continue
            if re.fullmatch(r"\d+(?:\.\d+)?(kg|g|ml|l|êµ¬|íŒ|ëª¨|í¬ê¸°|ë‹¨)", lowered):
                continue
            if len(token) >= 2:
                return token
    return None


def _extract_brand(text: str) -> str | None:
    normalized = text.lower()
    for brand in sorted(KNOWN_BRANDS, key=len, reverse=True):
        if brand.lower() in normalized:
            return brand
    return None


def _extract_size(text: str, item_name: str | None, quantity: int) -> str | None:
    match = SIZE_PATTERN.search(text)
    if match:
        value = match.group(1)
        unit = match.group(2).lower()
        return f"{value}{unit}"

    # "ë‘ë¶€ í•œ ëª¨" ê°™ì€ í‘œí˜„ ë³´ì •
    if item_name == "ë‘ë¶€" and re.search(r"(í•œ|í•˜ë‚˜|ë‘|ë‘˜|ì„¸|ì…‹|ë„¤|ë„·)\s*ëª¨", text):
        return f"{quantity}ëª¨"

    if item_name and item_name in ITEM_LIQUID_DEFAULT_SIZE:
        return ITEM_LIQUID_DEFAULT_SIZE[item_name]
    return None


def _contains_add_details(text: str) -> bool:
    lowered = text.lower()
    return bool(
        QUANTITY_PATTERN.search(text)
        or KOREAN_QTY_PATTERN.search(text)
        or SIZE_PATTERN.search(text)
        or _extract_brand(text)
        or "ëƒ‰ë™" in lowered
        or "ì›ì‚°ì§€" in lowered
    )


def _infer_recent_item_from_messages(messages: list[BaseMessage]) -> str | None:
    if len(messages) <= 1:
        return None

    for msg in reversed(messages[:-1]):
        msg_type = getattr(msg, "type", "")
        if msg_type not in {"human", "user"}:
            continue
        content = str(getattr(msg, "content", "")).strip()
        if not content:
            continue
        item = _extract_item_name(content, allow_fallback=False)
        if item:
            return item
    return None


def _format_item_label(item_name: str, brand: str | None, size: str | None) -> str:
    parts = [brand, item_name, f"({size})" if size else None]
    return " ".join([part for part in parts if part]).strip()


def _build_item_followup(item_name: str) -> str:
    if item_name == "ì‚¼ê²¹ì‚´":
        return (
            "ì‚¼ê²¹ì‚´ ì°¾ì•˜ì–´ìš”! ğŸ¥©\n"
            "ë³´í†µ 500g~600gì„ ë§ì´ êµ¬ë§¤í•´ìš”.\n"
            "ì›í•˜ì‹œëŠ” ë¸Œëœë“œë‚˜ ìš©ëŸ‰ì„ ì•Œë ¤ì£¼ì‹œë©´ ë°”ë¡œ ë‹´ì•„ë“œë¦´ê²Œìš”.\n"
            "ì˜ˆ: 'í•œëˆ ì‚¼ê²¹ì‚´ 600g ë‹´ì•„ì¤˜', 'ì‚¼ê²¹ì‚´ 500g ì¶”ê°€í•´ì¤˜'"
        )
    if item_name == "ì°¸ì´ìŠ¬":
        return (
            "ì°¸ì´ìŠ¬ í™•ì¸í–ˆì–´ìš”. ğŸ¶\n"
            "ë„ìˆ˜/ìš©ëŸ‰ì„ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•˜ê²Œ ë‹´ì•„ë“œë¦´ê²Œìš”.\n"
            "ì˜ˆ: 'ì°¸ì´ìŠ¬ í›„ë ˆì‰¬ 360ml 2ë³‘', 'ì°¸ì´ìŠ¬ 640ml 1ë³‘'"
        )
    if item_name == "ë¹„íƒ€500":
        return (
            "ë¹„íƒ€500 í™•ì¸í–ˆì–´ìš”. ğŸ‹\n"
            "ê°œìˆ˜ ë˜ëŠ” ì„¸íŠ¸ ë‹¨ìœ„ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë°”ë¡œ ë‹´ì•„ë“œë¦´ê²Œìš”.\n"
            "ì˜ˆ: 'ë¹„íƒ€500 100ml 10ë³‘', 'ë¹„íƒ€500 1ë°•ìŠ¤'"
        )

    example_size = "500ml" if item_name in ITEM_LIQUID_DEFAULT_SIZE else "500g"
    return (
        f"'{item_name}' í™•ì¸í–ˆì–´ìš”.\n"
        "ë¸Œëœë“œë‚˜ ê·œê²©ì„ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•˜ê²Œ ë‹´ì•„ë“œë¦´ ìˆ˜ ìˆì–´ìš”.\n"
        f"ì˜ˆ: '{item_name} 1ê°œ', '{item_name} {example_size}'"
    )


async def recommender_node(state: ChatState) -> dict:
    """ì¶”ì²œ/ì œì•ˆ ë…¸ë“œ."""
    req = state["messages"][-1].content if state["messages"] else ""

    if "ì €ë…" in req or "ë­ ë¨¹" in req:
        msg = (
            "ì˜¤ëŠ˜ ì €ë…ìœ¼ë¡œëŠ” **ê¹€ì¹˜ì°Œê°œ** ì–´ë– ì„¸ìš”? ğŸ²\n\n"
            "í•„ìˆ˜ ì¬ë£Œ:\n"
            "- ê¹€ì¹˜ (1/4í¬ê¸°)\n"
            "- ë‘ë¶€ (1ëª¨)\n"
            "- ë¼ì§€ê³ ê¸° (200g)\n\n"
            "\"ê¹€ì¹˜ì°Œê°œ ì¬ë£Œ ë‹´ì•„ì¤˜\"ë¼ê³  ë§ì”€í•˜ì‹œë©´ í•œ ë²ˆì— ì¶”ê°€í•´ë“œë¦´ê²Œìš”!"
        )
    elif "ì‚¼ê²¹ì‚´" in req:
        msg = (
            "ì‚¼ê²¹ì‚´ì´ë‘ ê°™ì´ ë¨¹ìœ¼ë©´ ì¢‹ì€ ê²ƒë“¤ì´ì—ìš”! ğŸ¥©\n\n"
            "- ìŒˆì±„ì†Œ ì„¸íŠ¸\n"
            "- ëœì¥ (ìŒˆì¥)\n"
            "- ë§ˆëŠ˜\n"
            "- ì†Œì£¼ or ë§¥ì£¼\n\n"
            "ë‹´ì„ê¹Œìš”?"
        )
    else:
        msg = "ì–´ë–¤ ìš”ë¦¬ë¥¼ í•˜ì‹¤ ì˜ˆì •ì¸ê°€ìš”? ë ˆì‹œí”¼ì— ë§ì¶° ì¬ë£Œë¥¼ ì¶”ì²œí•´ë“œë¦´ê²Œìš”. ğŸ½ï¸"

    return {"final_response": msg}


async def clarifier_node(state: ChatState) -> dict:
    """ì¶”ê°€ ì •ë³´ ì§ˆì˜ ë…¸ë“œ."""
    return {
        "final_response": (
            "ì£„ì†¡í•´ìš”, ì •í™•íˆ ì–´ë–¤ ìƒí’ˆì„ ë§ì”€í•˜ì‹œëŠ” ê±´ê°€ìš”? ğŸ¤”\n"
            "ë¸Œëœë“œë‚˜ ê·œê²©ì„ ì•Œë ¤ì£¼ì‹œë©´ ë” ì˜ ì°¾ì•„ë“œë¦´ ìˆ˜ ìˆì–´ìš”.\n"
            "ì˜ˆ: 'ì„œìš¸ìš°ìœ  1L', 'í’€ë¬´ì› ë‘ë¶€ 300g'"
        )
    }


async def general_node(state: ChatState) -> dict:
    """ì¼ë°˜ ëŒ€í™” ë…¸ë“œ."""
    basket = get_basket_store(state.get("user_id", "unknown_user"))
    basket_count = len(basket.items)
    if basket_count > 0:
        items_str = ", ".join(i.item_name for i in basket.items[:5])
        msg = f"ì•ˆë…•í•˜ì„¸ìš”! ì§€ê¸ˆ ì¥ë°”êµ¬ë‹ˆì— {basket_count}ê°œ í’ˆëª©({items_str})ì´ ìˆì–´ìš”. ë¬´ì—‡ì„ ë” ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š"
    else:
        msg = "ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ ë˜‘ì¥ AI ë¹„ì„œì—ìš”. ì¥ë°”êµ¬ë‹ˆì— ë‹´ì„ í’ˆëª©ì„ ë§ì”€í•´ì£¼ì„¸ìš”!"
    return {"final_response": msg}


# â”€â”€ 4. Conditional Edge â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def route_intent(state: ChatState) -> Literal["modifier", "recommender", "clarifier", "general"]:
    """Intentì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œ ê²°ì •."""
    intent = state.get("intent", "general")
    mapping = {
        "modify": "modifier",
        "recommend": "recommender",
        "clarify": "clarifier",
        "general": "general",
    }
    return mapping.get(intent, "general")


# â”€â”€ 5. Graph ì¡°ë¦½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

workflow = StateGraph(ChatState)

workflow.add_node("analyzer", analyzer_node)
workflow.add_node("modifier", modifier_node)
workflow.add_node("recommender", recommender_node)
workflow.add_node("clarifier", clarifier_node)
workflow.add_node("general", general_node)

workflow.set_entry_point("analyzer")

workflow.add_conditional_edges(
    "analyzer",
    route_intent,
    {
        "modifier": "modifier",
        "recommender": "recommender",
        "clarifier": "clarifier",
        "general": "general",
    },
)

workflow.add_edge("modifier", END)
workflow.add_edge("recommender", END)
workflow.add_edge("clarifier", END)
workflow.add_edge("general", END)

# ì»´íŒŒì¼
agent_graph = workflow.compile()
